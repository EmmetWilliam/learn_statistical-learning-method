{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第5章 决策树\n",
    "\n",
    "配置环境：python 3.6\n",
    "\n",
    "代码全部测试通过。\n",
    "\n",
    "代码参考：https://github.com/fengdu78/lihang-code/blob/master/code/%E7%AC%AC5%E7%AB%A0%20%E5%86%B3%E7%AD%96%E6%A0%91(DecisonTree)/Decision%20Tree%20(ID3%20%E5%89%AA%E6%9E%9D)\n",
    "\n",
    "此文档方便阅读，若需要复制粘贴可以在当前目录中查看`DecisionTree_cut.py`\n",
    "\n",
    "**训练完成决策树之后进行剪枝**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建数据\n",
    "def create_data():\n",
    "    datasets = [['青年', '否', '否', '一般', '否'],\n",
    "                ['青年', '否', '否', '好', '否'],\n",
    "                ['青年', '是', '否', '好', '是'],\n",
    "                ['青年', '是', '是', '一般', '是'],\n",
    "                ['青年', '否', '否', '一般', '否'],\n",
    "                ['中年', '否', '否', '一般', '否'],\n",
    "                ['中年', '否', '否', '好', '否'],\n",
    "                ['中年', '是', '是', '好', '是'],\n",
    "                ['中年', '否', '是', '非常好', '是'],\n",
    "                ['中年', '否', '是', '非常好', '是'],\n",
    "                ['老年', '否', '是', '非常好', '是'],\n",
    "                ['老年', '否', '是', '好', '是'],\n",
    "                ['老年', '是', '否', '好', '是'],\n",
    "                ['老年', '是', '否', '非常好', '是'],\n",
    "                ['老年', '否', '否', '一般', '否'],\n",
    "                ['青年', '否', '否', '一般', '是'],\n",
    "                ]\n",
    "    labels = [u'年龄', u'有工作', u'有自己的房子', u'信贷情况', u'类别']\n",
    "    # 返回数据集和每个维度的名称\n",
    "    return datasets, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets, labels = create_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNode:\n",
    "    def __init__(self, root=None, feature_name=None, label=None, data=None):\n",
    "        \"\"\"\n",
    "        :param root root=False 是分类节点 root=True 是叶节点\n",
    "        :param feature_name feature_name 表示分类节点的分类名字\n",
    "        :param label 根节点的类别，如果还有类别不唯一，通常是分类到最后，最多的那个类别，如果不是叶节点\n",
    "        label 为 None\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.feature_name = feature_name\n",
    "        self.label = label\n",
    "        self.tree = {}\n",
    "        self.data = data\n",
    "#         print('data', data)\n",
    "\n",
    "    def add_node(self, val, node):\n",
    "        # 添加子节点\n",
    "        self.tree[val] = node\n",
    "    \n",
    "    def copy(self):\n",
    "        \"\"\"\n",
    "        决策树剪枝有个操作叫做预剪枝\n",
    "        剪枝以后检查如果不需要剪枝就还原\n",
    "        所以要把这个预剪枝的节点保留下来\n",
    "        \"\"\"\n",
    "        c = {\n",
    "            'root': self.root,\n",
    "            'feature_name': self.feature_name,\n",
    "            'label': self.label,\n",
    "            'tree': self.tree,\n",
    "            'data': self.data,\n",
    "        }\n",
    "        return c\n",
    "\n",
    "    def sons_merge(self):\n",
    "        \"\"\"\n",
    "        合并子节点\n",
    "        \n",
    "        剪枝是减去叶节点上面的节点\n",
    "        也就是说，这个分类节点会变成叶节点\n",
    "        之前被分类的子节点都会被合并\n",
    "        \"\"\"\n",
    "        print('合并子节点')\n",
    "        self.root = True\n",
    "        data = [s.data for s in self.tree.values()]\n",
    "        data = np.concatenate(data, axis=0)\n",
    "        self.data = data\n",
    "        y_train = pd.Series(data[:, -1])\n",
    "        label = y_train.value_counts().sort_values(ascending=False).index[0]\n",
    "        self.label = label\n",
    "\n",
    "    def reduction(self, c):\n",
    "        # 节点的还原\n",
    "        self.root = c['root']\n",
    "        self.feature_name = c['feature_name']\n",
    "        self.label = c['label']\n",
    "        self.tree = c['tree']\n",
    "        self.data = c['data']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTree:\n",
    "    def __init__(self, epsilon=0.1, alpha=0.1):\n",
    "        \"\"\"\n",
    "        :param epsilon 信息增益的阈值\n",
    "        如果最大信息增益小于阈值，就不继续分节点了\n",
    "        :param alpha 公式 5.14 中的alpha\n",
    "        \"\"\"\n",
    "        self.tree = None\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "\n",
    "    @staticmethod\n",
    "    def cal_ent(dataset):\n",
    "        \"\"\"\n",
    "        计算当前数据的熵值 公式 5.1\n",
    "        \n",
    "        :param dataset 每行是一个样本，每行最后一个数是样本的类别\n",
    "        \"\"\"\n",
    "        data_length = len(dataset)\n",
    "        labels = {}\n",
    "        for i in range(data_length):\n",
    "            label = dataset[i][-1]\n",
    "            if label not in labels:\n",
    "                labels[label] = 0\n",
    "            labels[label] += 1\n",
    "        ent = -sum([(p/data_length)*log(p/data_length, 2)\n",
    "                    for p in labels.values()])\n",
    "\n",
    "        return ent\n",
    "\n",
    "    def cond_ent(self, dataset, axis=0):\n",
    "        \"\"\"\n",
    "        计算条件熵 书上公式 5.5\n",
    "        \n",
    "        按照某一条件分类，计算每一个分类数据的熵\n",
    "        计算每一个类别占未分类之前的比例\n",
    "        将对应频率与熵相乘，最后求和\n",
    "        \n",
    "        :param axis 是类别\n",
    "        \"\"\"\n",
    "        data_length = len(dataset)\n",
    "        cla_via_feature = {}\n",
    "        for r in range(data_length):\n",
    "            f = dataset[r][axis]\n",
    "            if f not in cla_via_feature:\n",
    "                cla_via_feature[f] = []\n",
    "            cla_via_feature[f].append(dataset[r])\n",
    "        ent = sum([(len(p)/data_length)*self.cal_ent(p)\n",
    "                   for p in cla_via_feature.values()])\n",
    "        return ent\n",
    "\n",
    "    def info_gain(self, ent, c_ent):\n",
    "        \"\"\"\n",
    "        信息增益\n",
    "        \"\"\"\n",
    "        return ent - c_ent\n",
    "\n",
    "    def info_gain_train(self, dataset):\n",
    "        \"\"\"\n",
    "        遍历所有数据，找到最大的信息增益是以哪个类别分类的\n",
    "        \"\"\"\n",
    "        count = len(dataset[0]) - 1\n",
    "        # 计算熵\n",
    "        ent = self.cal_ent(dataset)\n",
    "        best_feature = []\n",
    "        for c in range(count):\n",
    "            c_info_gain = self.info_gain(ent, self.cond_ent(dataset, axis=c))\n",
    "            best_feature.append((c, c_info_gain))\n",
    "        best_ = max(best_feature, key=lambda x: x[-1])\n",
    "\n",
    "        return best_\n",
    "\n",
    "    def train(self, train_data):\n",
    "        \"\"\"\n",
    "        递归函数\n",
    "        1. 先得到数据\n",
    "            数据格式是 pandas\n",
    "        2. 判断是否是叶节点\n",
    "            1. 所有分类的样本的标签是一样的\n",
    "            2. 虽然有的标签不一样，但是特征值用完了，现在强行弄成标签一样，类别最多的那个\n",
    "        3. 计算经验熵和条件经验熵，计算信息增益，算出信息增益最大的那个\n",
    "            1. 如果最大信息增益小于阈值，强行弄成标签一样，类别最多的那个\n",
    "        4. 把由 3 产生的数据重复执行前三步\n",
    "        \"\"\"\n",
    "        y_train, features = train_data.iloc[:, -1], train_data.columns[:-1]\n",
    "        # 如果是都是一样的类，返回叶节点\n",
    "        if len(y_train.value_counts()) == 1:\n",
    "            return DNode(root=True, label=y_train.iloc[0], data=np.array(train_data))\n",
    "\n",
    "        # 没有可分的特征值了，返回\n",
    "        if len(features) == 0:\n",
    "            return DNode(root=True,\n",
    "                         label=y_train.value_counts().sort_values(ascending=False).index[0], data=np.array(train_data))\n",
    "\n",
    "        # 计算经验熵和经验条件熵\n",
    "        max_feature, max_info_gain = self.info_gain_train(np.array(train_data))\n",
    "        max_feature_name = features[max_feature]\n",
    "\n",
    "        if max_info_gain < self.epsilon:\n",
    "            return DNode(root=True,\n",
    "                         label=y_train.value_counts().sort_values(ascending=False).index[0], data=np.array(train_data))\n",
    "\n",
    "        # 构建子集\n",
    "        node_tree = DNode(root=False,\n",
    "                          feature_name=max_feature_name)\n",
    "\n",
    "        # 将那些数据按 max_feature_name 分开\n",
    "        feature_list = train_data[max_feature_name].value_counts().index\n",
    "\n",
    "        for f in feature_list:\n",
    "            sub_train_df = train_data.loc[train_data[max_feature_name] == f]\n",
    "            sub_tree = self.train(sub_train_df)\n",
    "            node_tree.add_node(f, sub_tree)\n",
    "\n",
    "        return node_tree\n",
    "\n",
    "    def fit(self, train_data):\n",
    "        self.tree = self.train(train_data)\n",
    "        print('train done!')\n",
    "        return self.tree\n",
    "    \n",
    "    def cut(self, alpha):\n",
    "        print('尝试剪枝')\n",
    "        self.alpha = alpha if alpha else self.alpha\n",
    "        min_loss = self.cal_loss()\n",
    "        print('当前损失', min_loss)\n",
    "        def travel(node):\n",
    "            # 在当前作用域中使用上面定义的 min_loss\n",
    "            nonlocal min_loss\n",
    "            if not node.root:\n",
    "                # 找到叶子节点的父节点\n",
    "                son_nodes = list(node.tree.values())\n",
    "                roots = [n.root for n in son_nodes]\n",
    "                if not False in roots:\n",
    "                    print('找到全是叶节点的分类结点')\n",
    "                    # 全部是叶子节点\n",
    "                    # 删除节点之前，保存节点\n",
    "                    old_node = node.copy()\n",
    "                    # 合并节点\n",
    "                    node.sons_merge()\n",
    "                    # 计算新的损失函数\n",
    "                    new_loss = self.cal_loss()\n",
    "                    print('剪枝后的损失', new_loss)\n",
    "                    if new_loss < min_loss:\n",
    "                        # 1 代表剪枝\n",
    "                        print('剪枝')\n",
    "                        min_loss = new_loss\n",
    "                        return 1\n",
    "                    else:\n",
    "                        node.reduction(old_node)\n",
    "                else:\n",
    "                    # 子节点不全是叶节点\n",
    "                    i = 0\n",
    "                    re = 0\n",
    "                    while i < len(son_nodes):\n",
    "                        node = son_nodes[i]\n",
    "                        # travel 一共有三种可能的结果\n",
    "                        # 0 没有剪枝 最简单，什么都不动\n",
    "                        # 1 当前节点的一个节点剪枝了，那么当前节点有可能变成一个都是叶节点的分类结点\n",
    "                        # 2 当前节点收到了一个 2。告诉当前节点，你有一个子节点的子节点剪枝了\n",
    "                        # 你的那个子节点可能变成了一个全是叶节点的分类结点，所以你要重新检查这个子节点\n",
    "                        if_re = travel(node)\n",
    "                        if if_re == 1:\n",
    "                            # 此节点是儿子都是叶节点，并且进行了剪枝\n",
    "                            re = 1\n",
    "                        elif if_re == 2:\n",
    "                            # 重新检查当前子节点\n",
    "                            i -= 1\n",
    "                        i += 1\n",
    "                    if re:\n",
    "                        return 2\n",
    "            return 0\n",
    "        travel(self.tree)\n",
    "    \n",
    "    def all_leaves(self):\n",
    "        \"\"\"\n",
    "        得到所有的叶子节点\n",
    "        \"\"\"\n",
    "        leaves = []\n",
    "        tree = self.tree\n",
    "        def travel(node):\n",
    "            if node.root ==  False:\n",
    "                for n in node.tree.values():\n",
    "                    travel(n)\n",
    "            else:\n",
    "                leaves.append(node)\n",
    "        travel(tree)\n",
    "        \n",
    "        return leaves\n",
    "\n",
    "    def cal_loss(self):\n",
    "        \"\"\"\n",
    "        计算损失书上公式 5.13\n",
    "        \"\"\"\n",
    "        leaves = self.all_leaves()\n",
    "        first = []\n",
    "        for _l in leaves:\n",
    "            ent = self.cal_ent(_l.data)\n",
    "            first.append(ent)\n",
    "        res = sum(first) + self.alpha * len(leaves)\n",
    "        return res\n",
    "    \n",
    "    def predict(self, X, labels):\n",
    "        \"\"\"\n",
    "        :param X 待预测的数据\n",
    "        :labels 每个带预测数据的含义\n",
    "        \"\"\"\n",
    "        n = self.tree\n",
    "        while(not n.root):\n",
    "            feature_name = n.feature_name\n",
    "            idx = labels.index(feature_name)\n",
    "            cls = X[idx]\n",
    "            n = n.tree[cls]\n",
    "        \n",
    "        # n.root 现在是 False\n",
    "        # 输出 n.label 就是 类别\n",
    "        print('predict class ', n.label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train done!\n",
      "尝试剪枝\n",
      "当前损失 1.5709505944546687\n",
      "找到全是叶节点的分类结点\n",
      "合并子节点\n",
      "剪枝后的损失 1.4182958340544896\n",
      "剪枝\n",
      "找到全是叶节点的分类结点\n",
      "合并子节点\n",
      "剪枝后的损失 0.9500224216483542\n",
      "剪枝\n",
      "找到全是叶节点的分类结点\n",
      "合并子节点\n",
      "剪枝后的损失 1.1774178175281715\n",
      "predict class  否\n"
     ]
    }
   ],
   "source": [
    "data_df = pd.DataFrame(datasets, columns=labels)\n",
    "dt = DTree()\n",
    "dt.fit(data_df)\n",
    "# 进行减枝\n",
    "dt.cut(alpha=0.1)\n",
    "\n",
    "p_x = ['青年', '否', '否', '一般', '否']\n",
    "dt.predict(p_x, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
